\chapter{Implementierung}

Die agentenbasierte Modell wurde mit der Bibliothek Agents.jl\footnote{\texttt{https://juliadynamics.github.io/Agents.jl/stable/}} für Julia\footnote{\texttt{https://julialang.org/}} implementiert. Die verwendete Version von Agents.jl ist ein weitereintwickelter Fork der Verion 3.0.0.
	
Sämtliche in dieser Arbeit entwickelte Funktionalität ist in dem Paket \texttt{VotingProtocols}\footnote{\texttt{https://github.com/melomys/voting-protocols}} zusammengefasst.


\section{agent*innenbasierte Modellierung von Votingsystemen}

Die User*innen werden als Agent*innen des Modells modelliert. Posts, und die weiteren in Kapitel \ref{chapter:modell} beschriebenen Parameter, werden als Parameter des Modells von Agents.jl gespeichert. In der Implementierung stehen zwei Votingsysteme zur Verfügung:

\begin{table}[!htbp]
	\begin{tabularx}{\textwidth}{llX}
		\texttt{upvote\_system}& erlaubt Upvotes & $V_N = 1$\\
		\texttt{up\_and\_downvote\_system} & erlaubt Up- und Downvotes & $V_N =2$ \\
	\end{tabularx}
\end{table}

Das Modell wird für eine festgelegte Anzahl an Iterationen berechnet. Der Ablauf einer Simulation ist in Algorithm \ref{alg:model} beschrieben. Das Modell wird mit $U_N$ User*innen und ${P_{start}}_{N}$ Posts und sämtlichen weiteren Modellparametern initialisiert. In jedem Iterationsschritt wird für alle User*innen die definierte Agent*innenschrittfunktion (Abschnitt \ref{agentstep}) ausgeführt. Diese legt das Bewertungsverhalten der User*innen fest und ist für jedes Votingsystem unterschiedlich. Nach der Berechnung der User*innenaktionen wird in jedem Iterationsschritt die Modellschrittfunktion (Abschnitt \ref{modelstep}) ausgeführt und die definierten Iterationsevaluationsfunktionen (Abschnitt \ref{sec:auswertung}) aufgerufen. In dieser Funktion wird die Bewertungsmetrik angewendet und die Posts entsprechend angeordnet. Nach dem Durchlauf aller Iterationen werden schließlich die definierten Modellevaluationsfunktionen (Abschnitt \ref{sec:auswertung}) aufgerufen.

\captionsetup[figure]{name=Algorithmus}
\begin{algorithm}
			\caption{Modellsimulation (vereinfacht)}
	\begin{algorithmic}
			\State Erstelle Modell aus Modellkonfiguration
			\ForAll{Iterationen}
				\ForAll{Agent*innen}
					\State Agent*innenschrittfunktion für Agent*in
				\EndFor
				\State Modellschrittfunktion
				\State Rufe Iterationsevaluationsfunktionen auf
			\EndFor
			\State Rufe Modellevaluationsfunktionen auf
	\end{algorithmic}
	\label{alg:model}
\end{algorithm}



\subsection{Agent*innenschrittfunktion}
\label{agentstep}

Die Agent*innenschrittfunkion wird in Algorithm \ref{alg:agentstep} beschrieben. 
Für ein*e User*in $U$ wird zuerst berechnet, ob sie in der aktuellen Iteration aktiv ist. Dies wird anhand der Aktivitätswahrscheinlichkeit $a_U$ berechnet. Ist $U$ aktiv, werden so viele Posts von $U$ auf der Plattform betrachtet, wie durch die Konzentration $k_U$ vorgegeben sind. Für jeden Post $P$ bildet sich $U$ mit der User*innenmeinungsfunktion den Meinungswert $m_{P,U}$ und bewertet $P$, wenn die in Abschnitt \ref{bewertungszufriedenheit} beschrieben Bedingungen erfüllt sind.

\begin{algorithm}
	\caption{Agent*innenschritt}
		\label{alg:agentstep}
	\begin{algorithmic}
		\If{User*in ist aktiv}
		\ForAll{Posts in Konzentrationsspanne}
		\State Betrachten und eventuell Bewerten des Posts
		\EndFor
		\EndIf
	\end{algorithmic}
\end{algorithm}

\subsection{Modellschrittfunktion}
\label{modelstep}

Der Ablauf der Modellschrittfunktion ist in Algorithm \ref{alg:modelstep} dargestellt.
Für jeden Post $P$ wird mit der Bewertungsmetrik $s_P = B(P,i_M)$ berechnet. Anschließend werden ${P_N}_{iter}$ neue Posts dem Modell hinzugefügt. Falls es erwünscht ist, werden im nächsten Schritt die Postscores mit zufälligem Lärm verunreinigt. Nun können die Posts nach ihren Scores sortiert werden und die Modelliteration ist abgeschlossen.

%\newenvironment{malgorithm}[1][htb]
%{\renewcommand{\algorithmcfname}{Algorithmus}% Update algorithm name
%	\begin{algorithm}[#1]%
%	}{\end{algorithm}}

\begin{algorithm}
	\caption{Modellschritt}
		\label{alg:modelstep}
	\begin{algorithmic}
		\ForAll{Posts}
			\State Postscore mit Bewertungsmetrik berechnen
		\EndFor
		\State Neue Posts hinzufügen
		\If{Mit zufälliger Abweichung}
			\ForAll{Posts}
				\State Postscore mit zufälliger Abweichung verunreinigen
			\EndFor
		\EndIf
		\State Posts nach Postscore sortieren
	\end{algorithmic}
\end{algorithm}

%Julia ist eine High-Level Programmiersprache mit einer Ausführungsgeschwindigkeit, die an die statisch-typisierter Programmiersprachen wie C heranreicht.

%Julia verfügt über weitreichende Bibliothek zur Datenanalyse und einen Read-Eval-Print-Loop zur direkten und interaktiven Entwicklung von Code.

%\section{Agents.jl}

%Für Julia exisiert die weitentwickelte Framework \texttt{Agents.jl} für agentenbasierte Modellierung.
%Die Agenten in \texttt{Agents.jl} werden durch Julia Objekte beschrieben. Das Verhalten der Agenten wird über Schrittfunktion definiert. Wird das Modell ausgeführt wird für eine angegebene Menge an Iterationen für jeden Agenten die Schrittfunktion ausgeführt. Außerdem ist es möglich für eine Modellfunktion zu definieren, welche jeweils am Ende jeder Iteration ausgeführt wird. Es können beliebige Modellparameter definiert werden, welche sowohl in der Agenten- als auch in der Modellschrittfunktion gelesen und geändert werden können. Zur Datenkollektion werden Funktion übergeben, welche als einzigen Parameter das Modell erhalten. Nach jeder Iteration werden sämtliche Evaluationsfunktionen auf das Modell angewendet und die Ergebnisse in einem Feld gespeichert.

%Die in dieser Arbeit verwendte Version von \texttt{Agents.jl} ist ein weiterentwickelter Fork der Version \texttt{3.0.0}. Es wurden kleine Änderungen an der Kollektion von Modellparametern vorgenommen.
\cleardoublepage
\section{Erstellung von Modellkonfigurationen}
%TODO anpassen auf die aktuellen Namen

Um unterschiedliche Modellkonfigurationen zu testen und zu vergleichen wurde in dieser Arbeit ein Framework entwickelt, das es ermöglicht, modular Modellparameter zu Modellkonfigurationen zusammenzustellen. Die Modellkonfigurationen werden über eine Liste von Tupeln in Julia definiert.

Eine bespielhafte Konfiguration ist im Listing \ref{conf} in der Liste \texttt{model\_configs} zu sehen.

\lstset{
	basicstyle=\ttfamily,
	numbers=left, 
	numberstyle = \tiny, 
	stepnumber = 2,	
	moredelim=**[is][\color{JuliaBlue}]{\&}{\&},
	moredelim=**[is][\color{JuliaBeige}]{\$}{\$},
}
\captionsetup[figure]{name=Listing}

\begin{figure}[!h]

	\lstinputlisting{../scripts/experiments/example.jl}
		\caption{Definition von Modellkonfigurationen in Julia}
	\label{conf}
\end{figure}

Der erste Eintrag der Tupel definiert das Votingsystem. Durch ein Votingsystem wird ebenfalls eine Grundkonfiguration \ref{fig:grundconfig} definiert, diese stellt sicher, dass alle in der Simulation notwendigen Parameter gesetzt sind. In Zeile 3 wird das \texttt{up\_and\_downvote\_system} ausgewählt. Im zweiten Eintrag können über ein Dictionary einzelne Modellparameter der Grundkonfiguration überschrieben werden. In den Zeilen 5 bis 8 werden einzelne Modellparameter überschrieben. Die Modellparameter werden als Symbole angesteuert. Der Modellparameter \texttt{deviation\_function} wird mehrdeutig überschrieben, indem er durch eine Liste angegeben wird. Für jeden überschreibenden Wert wird eine eigene Modellkonfiguration angelegt. Das erste Tupel definiert somit zwei Modellkonfigurationen, die sich nur in \texttt{deviation\_function} unterscheiden. 

Der erste Eintrag des zweiten Tupels in Zeile 12 ist eine Liste aus Votingsystemen. Für jede der angegebenen Votingsysteme werden die im zweiten Eintrag des Tupels definierten Modellparameter überschrieben. Die in Zeile 14 bis 17 angegeben Modellparameter werden mehrdeutig überschrieben. Der Parameter \texttt{rating\_metric} wird doppelt definiert und \texttt{gravity} vierfach. Dadurch werden pro Votingsystem $2 * 4 = 8$ Modellkonfigurationen erstellt. Da zwei Votingsysteme angegeben sind, werden durch das zweite Tupel insgesamt $2 * (2* 4) = 16$ Modellkonfigurationen festgelegt. 

Das dritte Tupel in Zeile 21 besitzt als ersten Eintrag das Symbol \texttt{:all\_models}. Damit wird festgelegt, dass die im zweiten Eintrag des Tupels überschriebenen Modellparameter auf alle definierten Modellkonfigurationen angewendet werden.
Somit erhalten alle bereits definierten Modellkonfigurationen den Parameter \texttt{user\_opinion\_function} mit \texttt{consensus} zugewiesen.

Insgesamt wurden durch \texttt{model\_config} 18 Modellkonfigurationen festgelegt. Zwei im ersten Tupel und 16 im zweiten Tupel.


\section{Auswertung der Modelle}
\label{sec:auswertung}

Die Modelle werden durch die Angabe von Evaluationsmethoden aus Kapitel \ref{chap:evaluationsmethode} ausgewertet.

Die Iterations- und Modellevaluationsfunktionen werden über zwei Listen festgelegt. Die Evaluationsergebnisse werden analog in zwei Dataframes unter dem jeweiligen Namen der Evaluationsfunktion gespeichert.

Modellevaluationsmethoden haben Zugriff auf das berechnete Dataframe der Iterationsevaluationsmethoden.

Beispielhafte Konfigurationen der beiden Listen sind in Listing \ref{lst:eval_param} zu sehen. 

Die Iterationsevaluationsfunktionen \texttt{ndcg} und \texttt{gini} berechnen für jede Modelliteration den $nDCG$ bzw. Gini-Koeffizienten $G$ der Bewertungsmetrik. 


In der Liste \texttt{iter\_eval\_functions} sind die Iterationsevaluationsfunktionen festgelegt. Die Funktion \texttt{posts\_with\_no\_views} berechnet $P_{v = 0}$ des Models. Die Liste \texttt{model\_eval\_functions} definiert die Modellevaluationsparameter. Das Macro \texttt{@area\_under} aggregiert die übergeben Iterationsevaluationparameter mit der Trapezregel. Die durch das Macro erzeugte Funktion besitzt den Namen \texttt{area\_under\_<param>} In diesem Beispiel wird so über alle Iterationsevaluationsparameter aggregiert. Mit \texttt{@model\_parameter} können Modellparameter ausgewertet werden. Die erzeugte Funktion erhält den Namen des übergebenen Modellparameters. Hier werden der Initalscore der Posts, die Bewertungsmetrik, die Relevanzgravität und die User*innenmeinungsfunktion ausgewertet.


\begin{figure}[!h]
	\lstinputlisting{../scripts/experiments/eval.jl}
		\caption{Definition der Evaluationfunktionen}
	\label{lst:eval_param}
\end{figure}


\captionsetup[figure]{name=Abbildung}

\paragraph{Simulation und Export}
Die Modellkonfigurationen und die Evaluationsfunktionen können der Funktion \texttt{export\_data} übergeben werden. Diese führt die Berechnung und Evaluation des Modells aus und exportiert die Daten in das \texttt{rds}-Format Datenformat von R\footnote{\texttt{https://www.r-project.org/}}. Die Datenanalyse kann so in R erfolgen.


